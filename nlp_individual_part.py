# -*- coding: utf-8 -*-
"""NLP_Individual_Part.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11UGbJ39tS61yCGOfsODOwtfos8s3f1iY
"""

#!pip install -q datasets
#!pip install -q pytorch-transformers

import pandas as pd
import numpy as np

import torch
from pytorch_transformers import RobertaModel, RobertaTokenizer, RobertaForSequenceClassification, RobertaConfig

from datasets import load_dataset
from sklearn import metrics

from matplotlib import pyplot as plt
from collections import Counter
#import seaborn as sn

#from google.colab import drive
#drive.mount("/content/drive")

device = torch.device("cuda") if torch.cuda.is_available() else "cpu"

config = RobertaConfig.from_pretrained("roberta-base")
config.num_labels = 14

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

# Change commented line for training from scratch
model = RobertaForSequenceClassification(config)
#model = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=14, output_attentions = False, output_hidden_states = False)

model_path = "model_v3_final.pth"
model.load_state_dict(torch.load(model_path, map_location=device))


model = model.to(device)

# Loading in dataset and creating list of pre-selected labels to be used
dataset = load_dataset("go_emotions", "simplified")
print(dataset)

# Using index of keys instead of dataset's label number
used_labels = {0: "Admiration", 2: "Anger", 3: "Annoyance", 8: "Desire", 9: "Disappointment", 10: "Disapproval", 11: "Disgust", 14: "Fear", 18: "Love", 20: "Optimism", 22: "Realisation", 24: "Remorse", 26: "Surprise", 27: "Neutral"}

labels = list(used_labels.keys())

# Filtering dataset
edited_dataset = dataset.filter(lambda x: len(x["labels"]) == 1 and x["labels"][0] in labels)
print(edited_dataset.shape)



# Takes each sentence from the dataset and converts it into tokens whilst adding in the required special tokens
def tokenize(text, max_length = 50, cls_token = True, sep_token = True):

  tokens = tokenizer.tokenize(text)

  if len(tokens) > max_length:
        tokens = tokens[0:(max_length)]
  
  if cls_token:
      tokens.insert(0, tokenizer.cls_token)

  if sep_token:
      tokens.append(tokenizer.sep_token)

  input_ids = tokenizer.convert_tokens_to_ids(tokens)
  input_mask = [1] * len(input_ids)

  return torch.tensor(input_ids).unsqueeze(0), input_mask

# Subclass of the pytorch Dataset class, used to process data by the model
class CustomDataset(torch.utils.data.Dataset):
  
    def __init__(self, dataframe, labels):
        self.len = len(dataframe)
        self.data = dataframe
        self.labels = labels
        
    def __getitem__(self, index):
        text = self.data[index]["text"]
        tokens, _  = tokenize(text)
        label = self.data[index]["labels"][0]
        label = self.labels.index(label)

        return tokens, label
    
    def __len__(self):
        return self.len

# Setting the parameters for processing the data, before doing the actual processing
parameters = {"batch_size": 32, "shuffle": True, "drop_last": True, "num_workers": 0}

train_data = CustomDataset(edited_dataset["train"], labels)
test_data = CustomDataset(edited_dataset["test"], labels)
val_data = CustomDataset(edited_dataset["validation"], labels)

train_loader = torch.utils.data.DataLoader(train_data)
val_loader = torch.utils.data.DataLoader(val_data)
test_loader = torch.utils.data.DataLoader(test_data)

# Uncomment to combine training data with validation data
#train_loader = torch.utils.data.DataLoader(torch.utils.data.ConcatDataset([train_data, val_data]))

# Uncomment for half of training data split between validation and test data
#quarter = int(len(train_data)/4)
#train_val, train_test, train = torch.utils.data.random_split(train_data, [quarter, quarter, len(train_data) - 2*quarter])
#train_loader = torch.utils.data.DataLoader(train)
#val_loader = torch.utils.data.DataLoader(torch.utils.data.ConcatDataset([val_data, train_val]))
#test_loader = torch.utils.data.DataLoader(torch.utils.data.ConcatDataset([test_data, train_test]))

print(next(iter(train_loader))[0])
print(next(iter(train_loader))[0].squeeze(0))
print(tokenize("test")[0])

# tokenize [0], no squeeze
# May need _, predicted = torch.max(output, 1)

# Defining the loss function and optimizer as well as the learning rate for the model to use

# Uncomment loss function to use
loss_fn = torch.nn.CrossEntropyLoss()
#loss_fn = torch.nn.NLLLoss()


# Uncomment learning rate to use
lr = 1e-05
#lr = 1e-04
#lr = 1e-06

# Uncomment optimizer to use
optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)
#optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)
#optimizer = torch.optim.RMSprop(params=model.parameters(), lr=lr)

# Uncomment to skip cell and use other model training loop (For no validation data)
#%%script false --no-raise-error

version = 11
max_epochs = 1
model = model.train()



# Training
for epoch in range(max_epochs):
    print(f"EPOCH {epoch}")

    for iteration, (tokens, label) in enumerate(train_loader):
        optimizer.zero_grad()
        tokens = tokens.squeeze(0)

        tokens = tokens.to(device)
        label = label.to(device)

        output = model.forward(tokens)[0]
        _, predicted = torch.max(output, 1)
        
        loss = loss_fn(output, label)
        loss.backward()
        optimizer.step()

        # Loss and Accuracy calculations
        if iteration % 200 == 0 or iteration == 25383:

            #if iteration == 25383:
            #  torch.save(model.state_dict(), f"drive/My Drive/NLP Model Params/model_v{version}_epoch_{epoch}.pth")

            correct = 0
            total = 0

            for tokens, label in val_loader:

                tokens = tokens.squeeze(0)
                tokens = tokens.to(device)
                label = label.to(device)

                output = model.forward(tokens)[0]
                _, predicted = torch.max(output.data, 1)

                total += label.size(0)
                correct += (predicted.cpu() == label.cpu()).sum()

            accuracy = 100.00 * correct.numpy() / total
            print(f"Iter: {iteration}. Val Loss: {loss.item():.3f}. Val Accuracy: {accuracy:.3f}%")
            break

# Commented out IPython magic to ensure Python compatibility.
# # Comment out to use this cell's training loop, for no validation data
# %%script false --no-raise-error
# version = 11
# max_epochs = 2
# model = model.train()
# 
# #Adjusted model training for use without validation data
# for epoch in range(max_epochs):
#     print(f"EPOCH {epoch}")
#     
#     correct = 0
#     total = 0
#     for iteration, (tokens, label) in enumerate(train_loader):
# 
#         optimizer.zero_grad()
#         tokens = tokens.squeeze(0)
# 
#         tokens = tokens.to(device)
#         label = label.to(device)
# 
#         output = model.forward(tokens)[0]
#         _, predicted = torch.max(output, 1)
#         
#         loss = loss_fn(output, label)
#         loss.backward()
#         optimizer.step()
#         
#         total += label.size(0)
#         correct += (predicted.cpu() == label.cpu()).sum()
# 
#         # Uses training data for loss and accuracy calculations
#         if iteration % 6000 == 0 or iteration == 25383:
# 
#             if iteration == 25383:
#               torch.save(model.state_dict(), f"drive/My Drive/NLP Model Params/model_v{version}_epoch_{epoch}.pth")
# 
# 
#             accuracy = 100.00 * correct.numpy() / total
#             print(f"Iter: {iteration}. Train Loss: {loss.item():.3f}. Train Accuracy: {accuracy:.3f}%")
# 
#             correct = 0
#             total = 0

# Saving the weights of the model
#torch.save(model.state_dict(), f"drive/My Drive/NLP Model Params/model_v{version}_final.pth")

# Loading a model from previously saved weights
#model_path = f"drive/My Drive/NLP Model Params/model_v3_final.pth"
#model.load_state_dict(torch.load(model_path, map_location=device))

#import joblib

#joblib.dump(model, "model.joblib")



correct = 0
total = 0

true_labels = []
pred_labels = []

# Using the model to make predictions on the test data
for tokens, label in test_loader:

    tokens = tokens.squeeze(0)
    tokens = tokens.to(device)
    label = label.to(device)

    output = model.forward(tokens)[0]
    _, predicted = torch.max(output.data, 1)
    total += label.size(0)

    predicted = predicted.cpu()
    label = label.cpu()

    true_labels.extend(label)
    pred_labels.extend(predicted)

    correct += (predicted == label).sum()

accuracy = 100.00 * correct.numpy() / total
print(f"Test Accuracy: {accuracy:.3f}%")

label_names = [used_labels[i] for i in labels]

# Generate and display classification report
#classif_report = metrics.classification_report(true_labels, pred_labels, target_names=label_names)
#print(classif_report)

# Generate and display confusion matrix
#confusion = metrics.confusion_matrix(true_labels, pred_labels)
#confusion_data = pd.DataFrame(confusion / np.sum(confusion, axis=1)[:, None], index = label_names, columns = label_names)

#plt.figure(figsize = (12, 7))
#sn.heatmap(confusion_data, annot=True)